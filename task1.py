import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
from scipy.stats import pearsonr
from scipy.stats import multivariate_normal
import warnings
from matplotlib.pylab import mpl
mpl.rcParams['font.sans-serif'] = ['SimHei']
mpl.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore')


# --------------------------
# 1. æ•°æ®åŠ è½½ä¸æ¸…æ´—ï¼ˆå«èˆè¹ˆå¯¹å†³åŠ åˆ†æ•´åˆï¼‰
# --------------------------
def load_dancing_data(dance_bonus_dict=None):
    df = pd.read_excel("2026_MCM_Problem_C_Data.xlsx", sheet_name="Sheet1")
    judge_cols = [col for col in df.columns if "judge" in col and "score" in col and "week" in col]
    df[judge_cols] = df[judge_cols].fillna(0)
    K = len(judge_cols)
    if dance_bonus_dict is None:
        dance_bonus_dict = {}

    for week in range(1, 12):
        week_cols = [col for col in judge_cols if f"week{week}" in col]
        if week_cols:
            df[f"week{week}_judge_total_base"] = df[week_cols].sum(axis=1)
            bonus = 0
            for (s, w), val in dance_bonus_dict.items():
                if int(w) == week:
                    bonus = val
                    break
            df[f"week{week}_judge_total"] = df[f"week{week}_judge_total_base"] + bonus
            df[f"week{week}_judge_rank"] = df.groupby("season")[f"week{week}_judge_total"].rank(
                ascending=False, method="dense"
            ).fillna(0).astype(int)
    return df, K


# --------------------------
# 2. ä¸€è‡´æ€§æ£€éªŒ
# --------------------------
def consistency_check(result):
    J = result[result.columns[1]].values
    F = result[result.columns[2]].values
    corr, p_value = pearsonr(J, F)
    corr = np.abs(corr)
    n = len(result)
    RI_dict = {3: 0.58, 4: 0.90, 5: 1.12, 6: 1.24, 7: 1.32, 8: 1.41, 9: 1.45, 10: 1.49,
               11: 1.51, 12: 1.54, 13: 1.56, 14: 1.57, 15: 1.59}
    RI = RI_dict.get(n, 1.0)
    CI = 0 if corr == 1 else (1 - corr) / (n - 1)
    CR = CI / RI if RI != 0 else 0.0
    if CR < 0.1:
        level = "ä¼˜ç§€"
    elif CR < 0.2:
        level = "è‰¯å¥½"
    else:
        level = "è¾ƒå·®"
    return {
        "CR": round(CR, 3),
        "Pearson_corr": round(corr, 3),
        "p_value": round(p_value, 3),
        "consistency_level": level
    }


# --------------------------
# 3. ç›®æ ‡å‡½æ•°
# --------------------------
def objective(x, J):
    corr, _ = pearsonr(x, J)
    corr_term = corr if not np.isnan(corr) else 0.0
    x_norm = x / x.sum()
    entropy = -np.sum(x_norm * np.log(x_norm + 1e-10))
    var_x = np.var(x)
    x_range = np.max(x) - np.min(x)
    diversity = var_x / x_range if x_range != 0 else 0
    return -(0.6 * corr_term + 0.3 * entropy + 0.1 * diversity)


# --------------------------
# 4. çº¦æŸæ¡ä»¶ï¼ˆå¼ºåˆ¶è¢«æ·˜æ±°è€…ç»¼åˆæ’åæœ€å¤§ï¼‰
# --------------------------
def rank_sum_constraints(F, J, eliminated_idx):
    S = F + J
    return S[eliminated_idx] - np.max(S[np.arange(len(S)) != eliminated_idx]) - 1


# --------------------------
# 5. ç²‰ä¸æŠ•ç¥¨ä¼°ç®—ï¼ˆå«å¼ºåˆ¶çº¦æŸä¿®æ­£ï¼‰
# --------------------------
def estimate_fan_vote(df, season, week, eliminated_name):
    week_rank_col = f"week{week}_judge_rank"
    valid_data = df[(df["season"] == season) &
                    (df[week_rank_col] > 0) &
                    (df["celebrity_name"] != "Unknown")].copy()
    valid_data = valid_data.reset_index(drop=True)
    N = len(valid_data)
    if N <= 1:
        raise ValueError(f"ç¬¬{season}å­£ç¬¬{week}å‘¨æœ‰æ•ˆé€‰æ‰‹ä¸è¶³ï¼")

    eliminated_mask = valid_data["celebrity_name"] == eliminated_name
    if not eliminated_mask.any():
        raise ValueError(f"ç¬¬{season}å­£ç¬¬{week}å‘¨æœªæ‰¾åˆ°æ·˜æ±°è€…ï¼š{eliminated_name}")
    eliminated_local_idx = eliminated_mask.idxmax()

    J = valid_data[week_rank_col].values
    initial_F = J.copy()
    initial_F[eliminated_local_idx] = N

    constraints = [
        {"type": "ineq", "fun": rank_sum_constraints, "args": (J, eliminated_local_idx)},
        {"type": "ineq", "fun": lambda x: x - 1},
        {"type": "ineq", "fun": lambda x: N - x}
    ]
    bounds = [(1, N) for _ in range(N)]

    res = minimize(
        fun=objective,
        x0=initial_F,
        args=(J,),
        method="L-BFGS-B",
        bounds=bounds,
        constraints=constraints,
        options={"maxiter": 5000, "gtol": 1e-6, "disp": False}
    )

    fan_rank = pd.Series(res.x).rank(method="dense", ascending=True).astype(int).values
    valid_data[f"week{week}_fan_rank"] = fan_rank
    valid_data[f"week{week}_total_rank"] = valid_data[week_rank_col] + fan_rank

    # å¼ºåˆ¶ä¿®æ­£ï¼šç¡®ä¿è¢«æ·˜æ±°è€…ç»¼åˆæ’åä¸ºæœ€å¤§å€¼
    total_rank = valid_data[f"week{week}_total_rank"].values
    if total_rank[eliminated_local_idx] != np.max(total_rank):
        valid_data.loc[eliminated_local_idx, f"week{week}_fan_rank"] = N
        valid_data.loc[eliminated_local_idx, f"week{week}_total_rank"] = J[eliminated_local_idx] + N

    core_cols = ["celebrity_name", week_rank_col, f"week{week}_fan_rank", f"week{week}_total_rank"]
    return valid_data[core_cols], J, N, eliminated_local_idx


# --------------------------
# 6. è´å¶æ–¯é‡‡æ ·ï¼ˆÏƒ=0.8ï¼Œä¿è¯å¤šæ ·æ€§ï¼‰
# --------------------------
def bayesian_uncertainty(result, J, N, eliminated_local_idx, season, week, n_samples=300):
    fan_rank_point = result[result.columns[2]].values
    prior_std = 0.8  # å¢å¤§æ‰°åŠ¨å¹…åº¦ï¼Œä¿è¯é‡‡æ ·å¤šæ ·æ€§
    samples = []
    max_attempts = 15000
    attempts = 0

    while len(samples) < n_samples and attempts < max_attempts:
        F_perturb = np.round(fan_rank_point + multivariate_normal.rvs(mean=np.zeros(N), cov=np.eye(N) * prior_std ** 2))
        F_perturb = np.clip(F_perturb, 1, N).astype(int)
        F_perturb = pd.Series(F_perturb).rank(method="dense", ascending=True).astype(int).values
        S_perturb = F_perturb + J
        if S_perturb[eliminated_local_idx] == np.max(S_perturb):
            samples.append(F_perturb)
        attempts += 1

    if len(samples) < n_samples:
        samples += [fan_rank_point] * (n_samples - len(samples))
    samples = np.array(samples)

    stats = pd.DataFrame()
    stats["celebrity_name"] = result["celebrity_name"]
    stats["post_mean"] = np.mean(samples, axis=0).round(2)
    stats["post_std"] = np.std(samples, axis=0).round(2)
    stats["95%_lower"] = np.percentile(samples, 2.5, axis=0).round(0).astype(int)
    stats["95%_lower"] = np.maximum(stats["95%_lower"], 1)
    stats["95%_upper"] = np.percentile(samples, 97.5, axis=0).round(0).astype(int)
    stats["95%_upper"] = np.minimum(stats["95%_upper"], N)
    stats["CV"] = (stats["post_std"] / stats["post_mean"]).round(3)
    stats["CV"] = stats["CV"].fillna(0)

    plt.figure(figsize=(12, 7))
    for i, name in enumerate(stats["celebrity_name"]):
        plt.hist(samples[:, i], bins=np.arange(0.5, N + 1.5, 1), alpha=0.6, label=name)
    plt.title(f"ç¬¬{season}å­£ç¬¬{week}å‘¨ç²‰ä¸æ’åä¸ç¡®å®šæ€§åˆ†å¸ƒï¼ˆÏƒ={prior_std}ï¼‰", fontsize=14)
    plt.xlabel("ç²‰ä¸æ’åï¼ˆ1ä¸ºæœ€é«˜ï¼‰", fontsize=12)
    plt.ylabel("é‡‡æ ·é¢‘æ¬¡", fontsize=12)
    plt.xticks(range(1, N + 1))
    plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.tight_layout()
    plt.show()

    return stats, samples


# --------------------------
# 7. æŠŠæ¡ç¨‹åº¦åˆ†æ
# --------------------------
def confidence_analysis(stats, samples, consistency_result, eliminated_name, N, season, week):
    stats["conf_interval_width"] = stats["95%_upper"] - stats["95%_lower"]
    stats["sample_convergence"] = np.array([len(np.unique(samples[:, i])) / N for i in range(N)]).round(3)

    stats["CV_norm"] = 1 - np.clip(stats["CV"] / 0.3, 0, 1)
    stats["width_norm"] = 1 - np.clip(stats["conf_interval_width"] / N, 0, 1)
    stats["conv_norm"] = np.clip(stats["sample_convergence"], 0, 1)

    stats["confidence_score"] = (stats["CV_norm"] * 0.6 + stats["width_norm"] * 0.35 + stats["conv_norm"] * 0.05).round(
        3)

    def get_conf_level(score):
        if score >= 0.8:
            return "æé«˜"
        elif score >= 0.6:
            return "è¾ƒé«˜"
        elif score >= 0.4:
            return "ä¸­ç­‰"
        else:
            return "è¾ƒä½"

    stats["confidence_level"] = stats["confidence_score"].apply(get_conf_level)

    avg_conf_score = stats["confidence_score"].mean()
    cr = consistency_result["CR"]
    p_val = consistency_result["p_value"]
    if avg_conf_score >= 0.8 and cr < 0.1 and p_val < 0.05:
        overall_conf_level = "â˜…â˜…â˜…â˜…â˜… æé«˜æŠŠæ¡"
    elif avg_conf_score >= 0.6 and cr < 0.2:
        overall_conf_level = "â˜…â˜…â˜…â˜… è¾ƒé«˜æŠŠæ¡"
    elif avg_conf_score >= 0.4:
        overall_conf_level = "â˜…â˜…â˜… ä¸­ç­‰æŠŠæ¡"
    else:
        overall_conf_level = "â˜…â˜… è¾ƒä½æŠŠæ¡"

    support_metrics = {
        "å¹³å‡æŠŠæ¡åˆ†æ•°": round(avg_conf_score, 3),
        "ä¸€è‡´æ€§CRå€¼": cr,
        "ç›¸å…³æ€§På€¼": p_val,
        "å¹³å‡CVå€¼": round(stats["CV"].mean(), 3),
        "å¹³å‡ç½®ä¿¡åŒºé—´å®½åº¦": round(stats["conf_interval_width"].mean(), 1),
        "æ·˜æ±°è€…æŠŠæ¡ç¨‹åº¦": stats[stats["celebrity_name"] == eliminated_name]["confidence_level"].values[0],
        "æ·˜æ±°è€…æŠŠæ¡åˆ†æ•°": stats[stats["celebrity_name"] == eliminated_name]["confidence_score"].values[0]
    }

    plt.figure(figsize=(14, 6))
    color_map = {"æé«˜": "#2E8B57", "è¾ƒé«˜": "#4682B4", "ä¸­ç­‰": "#FFD700", "è¾ƒä½": "#DC143C"}
    bar_colors = [color_map[level] for level in stats["confidence_level"]]
    plt.bar(stats["celebrity_name"], stats["confidence_score"], color=bar_colors, alpha=0.8, edgecolor="black")
    plt.axhline(y=0.8, color="#2E8B57", linestyle="--", linewidth=1.5, label="æé«˜æŠŠæ¡(â‰¥0.8)")
    plt.axhline(y=0.6, color="#4682B4", linestyle="--", linewidth=1.5, label="è¾ƒé«˜æŠŠæ¡(â‰¥0.6)")
    plt.axhline(y=0.4, color="#FFD700", linestyle="--", linewidth=1.5, label="ä¸­ç­‰æŠŠæ¡(â‰¥0.4)")
    plt.ylim(0, 1.05)
    plt.title(f"ç¬¬{season}å­£ç¬¬{week}å‘¨ç²‰ä¸æ’åé¢„æµ‹æŠŠæ¡ç¨‹åº¦ï¼ˆæ•´ä½“ï¼š{overall_conf_level}ï¼‰", fontsize=14)
    plt.xlabel("å‚èµ›é€‰æ‰‹", fontsize=12)
    plt.ylabel("æŠŠæ¡ç¨‹åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰", fontsize=12)
    plt.xticks(rotation=45, ha="right")
    plt.legend(loc="lower right")
    plt.grid(axis="y", alpha=0.3)
    plt.tight_layout()
    plt.show()

    return stats, overall_conf_level, support_metrics


# --------------------------
# ä¸»å‡½æ•°
# --------------------------
if __name__ == "__main__":
    SEASON = 2
    WEEK = 4
    ELIMINATED_NAME = "Jerry Rice"
    N_SAMPLES = 300
    DANCE_BONUS_DICT = {}

    print("=" * 65)
    print("1. æ•°æ®åŠ è½½ä¸èˆè¹ˆå¯¹å†³åŠ åˆ†æ•´åˆ")
    print("=" * 65)
    df, K = load_dancing_data(DANCE_BONUS_DICT)
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼šå…±{len(df)}ä½é€‰æ‰‹ï¼Œ{K}ä½è¯„å§”ï¼Œèˆè¹ˆå¯¹å†³åŠ åˆ†è§„åˆ™ï¼š{DANCE_BONUS_DICT}")
    print(f"âœ… ç›®æ ‡åˆ†æï¼šç¬¬{SEASON}å­£ç¬¬{WEEK}å‘¨ï¼Œè¢«æ·˜æ±°é€‰æ‰‹ï¼š{ELIMINATED_NAME}\n")

    print("=" * 65)
    print("2. å¸¦æ·˜æ±°çº¦æŸçš„ç²‰ä¸æŠ•ç¥¨ä¼°ç®—")
    print("=" * 65)
    try:
        fan_vote_result, J, N, eliminated_idx = estimate_fan_vote(df, SEASON, WEEK, ELIMINATED_NAME)
        print("âœ… ç²‰ä¸æ’åä¼°ç®—å®Œæˆï¼Œç‚¹è§£ç»“æœï¼š")
        print(fan_vote_result.to_string(index=False))
    except Exception as e:
        print(f"âŒ ä¼°ç®—å¤±è´¥ï¼š{e}")
        exit()

    print("\n" + "=" * 65)
    print("3. æ¨¡å‹ä¸€è‡´æ€§æ£€éªŒï¼ˆCRå€¼ï¼‰")
    print("=" * 65)
    consistency_result = consistency_check(fan_vote_result)
    print(f"âœ… CRå€¼ï¼ˆä¸€è‡´æ€§æ¯”ç‡ï¼‰ï¼š{consistency_result['CR']}ï¼ˆ{consistency_result['consistency_level']}ï¼‰")
    print(f"âœ… Pearsonç›¸å…³ç³»æ•°ï¼š{consistency_result['Pearson_corr']}ï¼ˆè¶Šæ¥è¿‘1æ‹Ÿåˆè¶Šå¥½ï¼‰")
    print(f"âœ… ç›¸å…³æ€§På€¼ï¼š{consistency_result['p_value']}ï¼ˆ<0.05ä¸ºç»Ÿè®¡æ˜¾è‘—ï¼‰\n")

    print("=" * 65)
    print("4. è´å¶æ–¯ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆå…ˆéªŒæ ‡å‡†å·®Ïƒ=0.8ï¼‰")
    print("=" * 65)
    uncertainty_stats, samples = bayesian_uncertainty(fan_vote_result, J, N, eliminated_idx, SEASON, WEEK, N_SAMPLES)
    print("âœ… è´å¶æ–¯é‡‡æ ·å®Œæˆï¼ŒåéªŒç»Ÿè®¡ç»“æœï¼š")
    print(uncertainty_stats.to_string(index=False))

    print("\n" + "=" * 65)
    print("5. ä¼°ç®—ç»“æœæŠŠæ¡ç¨‹åº¦åˆ†æï¼ˆæ ¸å¿ƒï¼‰")
    print("=" * 65)
    conf_stats, overall_conf, support_metrics = confidence_analysis(uncertainty_stats, samples, consistency_result,
                                                                    ELIMINATED_NAME, N, SEASON, WEEK)
    print("âœ… å„é€‰æ‰‹æŠŠæ¡ç¨‹åº¦è¯¦æƒ…ï¼š")
    conf_detail = conf_stats[["celebrity_name", "confidence_score", "confidence_level", "CV", "conf_interval_width"]]
    print(conf_detail.to_string(index=False))
    print(f"\nğŸ“Š æ•´ä½“é¢„æµ‹æŠŠæ¡ç¨‹åº¦ï¼š{overall_conf}")
    print("ğŸ”‘ æ ¸å¿ƒæ”¯æ’‘æŒ‡æ ‡ï¼š")
    for k, v in support_metrics.items():
        print(f"   - {k}ï¼š{v}")

    print("\n" + "=" * 65)
    print("6. æ¨¡å‹æœ€ç»ˆç»¼åˆè¯„ä¼°")
    print("=" * 65)
    avg_CV = uncertainty_stats["CV"].mean()
    elim_total_rank = \
    fan_vote_result[fan_vote_result["celebrity_name"] == ELIMINATED_NAME][f"week{WEEK}_total_rank"].values[0]
    max_total_rank = fan_vote_result[f"week{WEEK}_total_rank"].max()
    print(f"âœ… å¹³å‡CVå€¼ï¼š{avg_CV:.3f} â†’ {'ç¨³å®š' if avg_CV < 0.2 else 'è¾ƒç¨³å®š' if avg_CV < 0.3 else 'ä¸ç¨³å®š'}")
    print(f"âœ… ä¸€è‡´æ€§ç­‰çº§ï¼š{consistency_result['consistency_level']}ï¼ˆCR={consistency_result['CR']}ï¼‰")
    print(
        f"âœ… æ·˜æ±°çº¦æŸæ»¡è¶³ï¼š{elim_total_rank == max_total_rank}ï¼ˆè¢«æ·˜æ±°è€…ç»¼åˆæ’å={elim_total_rank}ï¼Œæœ€å¤§å€¼={max_total_rank}ï¼‰")
    print(f"âœ… ç›¸å…³æ€§æ˜¾è‘—ï¼š{consistency_result['p_value'] < 0.05}")
    print(f"âœ… æ•´ä½“æŠŠæ¡ç¨‹åº¦ï¼š{overall_conf}")
    print("\n" + "=" * 65)
    print("ğŸ“Œ æ¨¡å‹è¿è¡Œå®Œæˆï¼Œæ‰€æœ‰ç»“æœå·²è¾“å‡ºå¹¶å¯è§†åŒ–ï¼")
    print("=" * 65)